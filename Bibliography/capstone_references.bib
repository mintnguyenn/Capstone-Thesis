@misc{Zhang2014,
  title={Computer Vision: A Reference Guide},
  author={Ikeuchi, Katsushi},
  year={2014},
  publisher={Springer Publishing Company, Incorporated}
}

@article{Korthals2019,
abstract = {Evaluation of robotic experiments requires physical robots as well as position sensing systems. Accurate systems detecting sufficiently all necessary degrees of freedom, like the famous Vicon system, are commonly too expensive. Therefore, we target an economical multi-camera based solution by following these three requirements: Using multiple cameras to track even large laboratory areas, applying fiducial marker trackers for pose identification, and fuse tracking hypothesis resulting from multiple cameras via extended Kalman filter (i.e. ROS's robot_localization). While the registration of a multi-camera system for collaborative tracking remains a challenging issue, the contribution of this paper is as follows: We introduce the framework of Cognitive Interaction Tracking (CITrack). Then, common fiducial marker tracking systems (ARToolKit, April-Tag, ArUco) are compared with respect to their maintainability. Lastly, a graph-based camera registration approach in SE(3), using the fiducial marker tracking in a multi-camera setup, is presented and evaluated.},
author = {Korthals, Timo and Wolf, Daniel and Rudolph, Daniel and Hesse, Marc and Ruckert, Ulrich},
doi = {10.1109/ECMR.2019.8870969},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korthals et al. - 2019 - Fiducial marker based extrinsic camera calibration for a robot benchmarking platform.pdf:pdf},
isbn = {9781728136059},
journal = {2019 European Conference on Mobile Robots, ECMR 2019 - Proceedings},
title = {{Fiducial marker based extrinsic camera calibration for a robot benchmarking platform}},
year = {2019}
}
@article{Liu2020,
abstract = {RGB-D cameras (or color-depth cameras) play key roles in many vision applications. A typical RGB-D camera has only rough intrinsic and extrinsic calibrations that cannot provide the accuracy required in many vision applications. In this paper, we propose a novel and accurate sphere-based calibration framework for estimating the intrinsic and extrinsic parameters of color-depth sensor pair. Additionally, a method of depth error correction is suggested, and the principle of error correction is analyzed in detail. In our method, the feature extraction module can automatically and reliably detect the center and edges of the sphere projection, while excluding noise data and outliers, and the projection of the sphere center on RGB and depth images is used to obtain a closed solution of the initial parameters. Finally, all the parameters are accurately estimated within the framework of nonlinear global minimization. Compared to other state-of-the-art methods, our calibration method is easy to use and provides higher calibration accuracy. Detailed experimental analysis is performed to support our conclusions.},
author = {Liu, Hongyan and Qu, Daokui and Xu, Fang and Zou, Fengshan and Song, Jilai and Jia, Kai},
doi = {10.1364/oe.392414},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2020 - Approach for accurate calibration of RGB-D cameras using spheres.pdf:pdf},
issn = {1094-4087},
journal = {Optics Express},
month = {jun},
number = {13},
pages = {19058},
pmid = {32672191},
publisher = {The Optical Society},
title = {{Approach for accurate calibration of RGB-D cameras using spheres}},
volume = {28},
year = {2020}
}

@article{Olson2011,
abstract = {While the use of naturally-occurring features is a central focus of machine perception, artificial features (fiducials) play an important role in creating controllable experiments, ground truthing, and in simplifying the development of systems where perception is not the central objective. We describe a new visual fiducial system that uses a 2D bar code style "tag", allowing full 6 DOF localization of features from a single image. Our system improves upon previous systems, incorporating a fast and robust line detection system, a stronger digital coding system, and greater robustness to occlusion, warping, and lens distortion. While similar in concept to the ARTag system, our method is fully open and the algorithms are documented in detail. {\textcopyright} 2011 IEEE.},
author = {Olson, Edwin},
doi = {10.1109/ICRA.2011.5979561},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/AprilTag A robust and ﬂexible visual ﬁducial system.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3400--3407},
title = {{AprilTag: A robust and flexible visual fiducial system}},
year = {2011}
}
@article{Baker2003,
abstract = {With the advent of laboratories containing dozens of cameras, and the possibility of laboratories containing hundreds of cameras, the question of how to calibrate all the cameras has become pressing. While it is certainly possible to calibrate these networks in a labor intensive manner, a simple, stable, and accurate calibration method is still needed. This paper presents such a method, based on textures printable on a laser printer and mounted on a board. We will show what the problems with the current methods are, and show how these problems can be overcome with a novel use of a trilinear constraint related to the vanishing point constraint, which we call the primsatic line constraint. High accuracy with little user effort is achieved with this method.},
author = {Baker, Patrick T. and Aloimonos, Yiannis},
doi = {10.1109/CVPRW.2003.10085},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Calibration of a Multicamera Network.pdf:pdf},
isbn = {0769519008},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {72},
title = {{Calibration of a Multicamera Network}},
volume = {7},
year = {2003}
}
@article{Garrido-Jurado2014,
abstract = {This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem. {\textcopyright} 2014 Elsevier Ltd.},
author = {Garrido-Jurado, S. and Mu{\~{n}}oz-Salinas, R. and Madrid-Cuevas, F. J. and Mar{\'{i}}n-Jim{\'{e}}nez, M. J.},
doi = {10.1016/J.PATCOG.2014.01.005},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Augmented reality,Computer vision,Fiducial marker},
month = {jun},
number = {6},
pages = {2280--2292},
publisher = {Pergamon},
title = {{Automatic generation and detection of highly reliable fiducial markers under occlusion}},
volume = {47},
year = {2014}
}
@article{Triggs1999,
abstract = {We describe two direct quasilinear methods for camera pose (absolute orientation) and calibration from a single image of 4 or 5 known 3D points. They generalize the 6 point `Direct Linear Transform' method by incorporating partial prior camera knowledge, while still allowing some unknown calibration parameters to be recovered. Only linear algebra is required, the solution is unique in non-degenerate cases, and additional points can be included for improved stability. Both methods fail for coplanar points, but we give an experimental eigendecomposition based one that handles both planar and nonplanar cases. Our methods use recent polynomial solving technology, and we give a brief summary of this. One of our aims was to try to understand the numerical behaviour of modern polynomial solvers on some relatively simple test cases, with a view to other vision applications.},
author = {Triggs, Bill},
doi = {10.1109/iccv.1999.791231},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Camera pose and calibration from 4 or 5 known 3D points.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {be implemented using the,bration models could easily,calibration,camera pose,direct linear transform,eigensystems,multi-solution meth-,multiresultants,polynomial solving,same,techniques,there are also associated},
pages = {278--284},
title = {{Camera pose and calibration from 4 or 5 known 3D points}},
volume = {1},
year = {1999}
}
@inproceedings{Heikkila1997,
author = {Heikkila, Janne and Silv{\'{e}}n, Olli},
booktitle = {Proceedings of IEEE computer society conference on computer vision and pattern recognition},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heikkila, Silv{\'{e}}n - 1997 - A four-step camera calibration procedure with implicit image correction.pdf:pdf},
isbn = {0818678224},
pages = {1106--1112},
publisher = {IEEE},
title = {{A four-step camera calibration procedure with implicit image correction}},
year = {1997}
}
@article{Reinke2019,
abstract = {Legged robots are becoming popular not only in research, but also in industry, where they can demonstrate their superiority over wheeled machines in a variety of applications. Either when acting as mobile manipulators or just as all-terrain ground vehicles, these machines need to precisely track the desired base and end-effector trajectories, perform Simultaneous Localization and Mapping (SLAM), and move in challenging environments, all while keeping balance. A crucial aspect for these tasks is that all onboard sensors must be properly calibrated and synchronized to provide consistent signals for all the software modules they feed. In this paper, we focus on the problem of calibrating the relative pose between a set of cameras and the base link of a quadruped robot. This pose is fundamental to successfully perform sensor fusion, state estimation, mapping, and any other task requiring visual feedback. To solve this problem, we propose an approach based on factor graphs that jointly optimizes the mutual position of the cameras and the robot base using kinematics and fiducial markers. We also quantitatively compare its performance with other state-of-the-art methods on the hydraulic quadruped robot HyQ. The proposed approach is simple, modular, and independent from external devices other than the fiducial marker.},
archivePrefix = {arXiv},
arxivId = {1811.01254},
author = {Reinke, Andrzej and Camurri, Marco and Semini, Claudio},
doi = {10.1109/IRC.2019.00071},
eprint = {1811.01254},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/A Factor Graph Approach to Multi-Camera Extrinsic Calibration on Legged Robots.pdf:pdf},
isbn = {9781538692455},
journal = {Proceedings - 3rd IEEE International Conference on Robotic Computing, IRC 2019},
keywords = {extrinsic calibration,kinematics,quadruped robots},
pages = {391--394},
title = {{A Factor Graph Approach to Multi-camera Extrinsic Calibration on Legged Robots}},
year = {2019}
}
@article{Lu2018,
abstract = {As there is a rapid development of robotics in the field of automation engineering, ego-motion estimation has become a most challenging task. In this review, we presented a model to help describe the PnP problems, and introduced two most common solutions. The P3P solution is the smallest subset of control points that yields a finite number of solutions. The EPnP solution is to reduce the complexity by expressing the n 3D points as a weighted sum of four virtual control points. The former solution is widely applied while there are 3 pairs of corresponding points in the problem. However, in most real cases, the latter is more used.},
author = {Lu, Xiao Xin},
doi = {10.1088/1742-6596/1087/5/052009},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/SolvePnP Problem.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {5},
title = {{A Review of Solutions for Perspective-n-Point Problem in Camera Pose Estimation}},
volume = {1087},
year = {2018}
}
@article{Kummerle2011,
abstract = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state-of-the-art approaches for the specific problems. {\textcopyright} 2011 IEEE.},
author = {K{\"{u}}mmerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
doi = {10.1109/ICRA.2011.5979949},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/G2O A general framework for graph optimization.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3607--3613},
title = {{G2o: A general framework for graph optimization}},
year = {2011}
}
@article{Tsai1987,
abstract = {A new technique for three-dimensional (3D) camera calibration for machine vision metrology using off-the-shelf TV cameras and lenses is described. The two-stage technique is aimed at efficient computation of camera external position and orientation relative to object reference coordinate system as well as the effective focal length, radial lens distortion, and image scanning parameters. The two-stage technique has advantage in terms of accuracy, speed, and versatility over existing state of the art. A critical review of the state of the art is given in the beginning. A theoretical framework is established, supported by comprehensive proof in five appendixes, and may pave the way for future research on 3D robotics vision. Test results using real data are described. Both accuracy and speed are reported. The experimental results are analyzed and compared with theoretical prediction. Recent effort indicates that with slight modification, the two-stage calibration can be done in real time. Copyright {\textcopyright} 1987 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Tsai, Roger Y.},
doi = {10.1109/JRA.1987.1087109},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/A versatile camera calibration technique for high accuracy 3D machine vision metrology using off the shelf TV cameras and lenses.pdf:pdf},
issn = {08824967},
journal = {IEEE Journal on Robotics and Automation},
number = {4},
pages = {323--344},
title = {{A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses}},
volume = {3},
year = {1987}
}
@article{Marcon2017,
abstract = {A multi-camera rig calibration algorithm based on a double sided planar target is proposed. Due to their inherently simple realisation, low cost and accuracy, planar calibration targets came out as one of the most largely adopted calibration tools both for intrinsic and extrinsic camera parameters. However, concerning the estimation of extrinsic parameters, one of the major drawbacks of these targets is their requirement for distinct target visibility from both cameras. This prevents many configurations from being adopted where, e.g. two cameras are facing each other. An inexpensive solution could be based on printing/pasting a planar pattern on both target sides, however, the relative misalignment between the patterns on the two sides and the target thickness could be unknown. The authors propose a solution where double-sided target displacement error is estimated together with the extrinsic parameters allowing the reuse of all the available planar calibration tools in less constrained configurations. To assess their approach the authors tested the system in two scenarios, one using two professional 4K cameras and one using two smartphones.},
author = {Marcon, Marco and Sarti, Augusto and Tubaro, Stefano},
doi = {10.1049/iet-cvi.2016.0193},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcon, Sarti, Tubaro - 2017 - Multicamera rig calibration by double-sided thick checkerboard.pdf:pdf},
issn = {17519640},
journal = {IET Computer Vision},
month = {sep},
number = {6},
pages = {448--454},
publisher = {Institution of Engineering and Technology},
title = {{Multicamera rig calibration by double-sided thick checkerboard}},
volume = {11},
year = {2017}
}
@article{Shen2009,
abstract = {Rapid calibration of multi-camera systems using planar targets is typically impractical. Here we report a nonplanar target for rapid calibration of inward-looking visual sensor networks (VSNs), designed using a novel VSN simulator and verified experimentally. The target consists of a large central sphere, surrounded by several smaller, rigidly mounted spheres. Rotation about an axis through the center of this sphere changes the location of the outer spheres, while keeping a fixed reference point in the center to permit extrinsic parameter extraction. Using the simulation, different target configurations are tested for field-of-view (FOV) coverage for each camera. Experimental results using a constructed operational prototype are consistent with traditional planar target calibrations, but require only a single view of the target to extract the intrinsic parameters. {\textcopyright}2009 IEEE.},
author = {Shen, Edward and Carr, G. Peter K. and Thomas, Paul and Hornsey, Richard},
doi = {10.1109/ICSENS.2009.5398433},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Non-planar target for multi camera network calibration.pdf:pdf},
isbn = {9781424445486},
journal = {Proceedings of IEEE Sensors},
pages = {1410--1414},
publisher = {IEEE},
title = {{Non-planar target for multi-camera network calibration}},
year = {2009}
}
@article{Ueshiba2003,
abstract = {A new calibration algorithm for multi-camera systems using a planar reference pattern is proposed. The algorithm is an extension of Sturm-Maybank-Zhang style plane-based calibration technique for use with multiple cameras. Rigid displacements between the cameras are recovered as well as the intrinsic parameters only by capturing with the cameras a model plane with known reference points placed at three or more locations. Thus the algorithm yields a simple calibration means for stereo vision systems with an arbitrary number of cameras while maintaining the handiness and flexibility of the original method. The algorithm is based on factorization of homography matrices between the model and image planes into the camera and plane parameters. To compensate for the indetermination of scaling factors, each homography matrix is rescaled by a double eigenvalue of a planar homology defined by two views and two model planes. The obtained parameters are finally refined by a non-linear maximum likelihood estimation (MLE) process. The validity of the proposed technique was verified through simulation and experiments with real data.},
author = {Ueshiba, Toshio and Tomita, Fumiaki},
doi = {10.1109/iccv.2003.1238453},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Plane-based calibration algorithm for multi-camera systems via factorization of homography matrices.pdf:pdf},
isbn = {0769519504},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {966--973},
title = {{Plane-based calibration algorithm for multi-camera systems via factorization of homography matrices}},
volume = {2},
year = {2003}
}
@article{Dong2016,
abstract = {In this paper, an extrinsic calibration method for a non-overlapping camera network is presented based on close-range photogrammetry. The method does not require calibration targets or the cameras to be moved. The visual sensors are relatively motionless and do not see the same area at the same time. The proposed method combines the multiple cameras using some arbitrarily distributed encoded targets. The calibration procedure consists of three steps: reconstructing the three-dimensional (3D) coordinates of the encoded targets using a hand-held digital camera, performing the intrinsic calibration of the camera network, and calibrating the extrinsic parameters of each camera with only one image. A series of experiments, including 3D reconstruction, rotation, and translation, are employed to validate the proposed approach. The results show that the relative error for the 3D reconstruction is smaller than 0.003%, the relative errors of both rotation and translation are less than 0.066%, and the re-projection error is only 0.09 pixels.},
author = {Dong, Shuai and Shao, Xinxing and Kang, Xin and Yang, Fujun and He, Xiaoyuan},
doi = {10.1364/ao.55.006363},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Extrinsic calibration of a non-overlapping camera network based on close-range photogrammetry.pdf:pdf},
issn = {0003-6935},
journal = {Applied Optics},
number = {23},
pages = {6363},
title = {{Extrinsic calibration of a non-overlapping camera network based on close-range photogrammetry}},
volume = {55},
year = {2016}
}
@article{Meijer,
author = {Meijer, Peter B L and Martiniere, Anthony and Antipolis, Sophia},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Multiple View Camera Calibration for Localization.pdf:pdf},
journal = {Camera},
pages = {228--234},
title = {{MULTIPLE VIEW CAMERA CALIBRATION FOR LOCALIZATION}}
}
@article{Zhang2000,
abstract = {We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use. The corresponding software is available from the author's Web page. {\textcopyright} 2000 IEEE.},
author = {Zhang, Zhengyou},
doi = {10.1109/34.888718},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/A flexible new technique for camera calibration.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {2d pattern,Absolute conic,Calibration from planes,Camera calibration,Closed-form solution,Flexible plane-based calibration,Flexible setup,Lens distortion,Maximum likelihood estimation,Projective mapping},
number = {11},
pages = {1330--1334},
title = {{A flexible new technique for camera calibration}},
volume = {22},
year = {2000}
}
@article{Pedemini1999,
author = {Pedemini, Federico and Tuburo, Stefuno},
file = {:F\:/CAPSTONE PROJECT UNIVERSITY/Papers/Multi camera systems.pdf:pdf},
number = {May},
title = {{Calibration and Application}},
year = {1999}
}
@article{Liu2020a,
abstract = {RGB-D cameras (or color-depth cameras) play key roles in many vision applications. A typical RGB-D camera has only rough intrinsic and extrinsic calibrations that cannot provide the accuracy required in many vision applications. In this paper, we propose a novel and accurate sphere-based calibration framework for estimating the intrinsic and extrinsic parameters of color-depth sensor pair. Additionally, a method of depth error correction is suggested, and the principle of error correction is analyzed in detail. In our method, the feature extraction module can automatically and reliably detect the center and edges of the sphere projection, while excluding noise data and outliers, and the projection of the sphere center on RGB and depth images is used to obtain a closed solution of the initial parameters. Finally, all the parameters are accurately estimated within the framework of nonlinear global minimization. Compared to other state-of-the-art methods, our calibration method is easy to use and provides higher calibration accuracy. Detailed experimental analysis is performed to support our conclusions.},
author = {Liu, Hongyan and Qu, Daokui and Xu, Fang and Zou, Fengshan and Song, Jilai and Jia, Kai},
doi = {10.1364/oe.392414},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2020 - Approach for accurate calibration of RGB-D cameras using spheres.pdf:pdf},
issn = {1094-4087},
journal = {Optics Express},
month = {jun},
number = {13},
pages = {19058},
pmid = {32672191},
publisher = {The Optical Society},
title = {{Approach for accurate calibration of RGB-D cameras using spheres}},
volume = {28},
year = {2020}
}

@article{Yin2018,
abstract = {Multi-camera systems are widely used in the fields of airborne remote sensing and unmanned aerial vehicle imaging. The measurement precision of these systems depends on the accuracy of the extrinsic parameters. Therefore, it is important to accurately calibrate the extrinsic parameters between the onboard cameras. Unlike conventional multi-camera calibration methods with a common field of view (FOV), multi-camera calibration without overlapping FOVs has certain difficulties. In this paper, we propose a calibration method for a multi-camera system without common FOVs, which is used on aero photogrammetry. First, the extrinsic parameters of any two cameras in a multi-camera system is calibrated, and the extrinsic matrix is optimized by the reprojection error. Then, the extrinsic parameters of each camera are unified to the system reference coordinate system by using the global optimization method. A simulation experiment and a physical verification experiment are designed for the theoretical arithmetic. The experimental results show that this method is operable. The rotation error angle of the camera's extrinsic parameters is less than 0.001rad and the translation error is less than 0.08 mm.},
author = {Yin, Lei and Wang, Xiangjun and Ni, Yubo and Zhou, Kai and Zhang, Jilong},
doi = {10.3390/rs10081298},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - 2018 - Extrinsic parameters calibration method of cameras with non-overlapping fields of view in airborne remote sensing.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Airborne remote sensing,Camera calibration,Extrinsic parameter,Non-overlapping field of view,Stereo vision},
mendeley-groups = {Multi-camera calibration/Non-overlapping FOV},
month = {aug},
number = {8},
publisher = {MDPI AG},
title = {{Extrinsic parameters calibration method of cameras with non-overlapping fields of view in airborne remote sensing}},
volume = {10},
year = {2018}
}

@article{Shen2011,
abstract = {The rapid calibration of multi-camera systems using a planar target is typically impractical due to the difficulty of viewing the target in each camera simultaneously. To address these short-comings, a complete calibration methodology using a novel non-planar target for rapid calibration of inward-looking visual sensor networks (VSNs) is presented. We discuss the practical limitations of the approach, arising from an analysis of implementation issues when using spheres as calibration targets such as target-to-target and target-to-camera orientation relationships. This procedure is applied to fully calibrate (intrinsic and extrinsic camera parameters) a twelve camera inward-looking VSN, using only a single image per camera. Results from the calibration are compared for nominal and measured dimensions of the target. {\textcopyright} 2011 IEEE.},
author = {Shen, Edward and Hornsey, Richard},
doi = {10.1109/JSEN.2011.2123884},
file = {:C\:/Users/nhatm/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen, Hornsey - 2011 - Multi-camera network calibration with a non-planar target.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sensors Journal},
keywords = {Calibration,camera,multi-camera network,sphere},
mendeley-groups = {Multi-camera calibration/Stereo Cameras},
number = {10},
pages = {2356--2364},
publisher = {IEEE},
title = {{Multi-camera network calibration with a non-planar target}},
volume = {11},
year = {2011}
}

